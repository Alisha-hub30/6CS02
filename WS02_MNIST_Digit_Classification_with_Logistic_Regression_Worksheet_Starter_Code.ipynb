{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpi5f-NuuRbg"
   },
   "source": [
    "##  **Some Helper Function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDqrxMpLuhLO"
   },
   "source": [
    "### Softmax Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "YoOjTJJpt6Nv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax probabilities for a given input matrix.\n",
    "\n",
    "    Parameters:\n",
    "    z (numpy.ndarray): Logits (raw scores) of shape (m, n), where\n",
    "                       - m is the number of samples.\n",
    "                       - n is the number of classes.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Softmax probability matrix of shape (m, n), where\n",
    "                   each row sums to 1 and represents the probability\n",
    "                   distribution over classes.\n",
    "\n",
    "    Notes:\n",
    "    - The input to softmax is typically computed as: z = XW + b.\n",
    "    - Uses numerical stabilization by subtracting the max value per row.\n",
    "    \"\"\"\n",
    "\n",
    "    # Your Code Here.\n",
    "    z_exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    softmax_probs = z_exp / np.sum(z_exp, axis=1, keepdims=True)\n",
    "    return softmax_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFnMdHJzrUJV"
   },
   "source": [
    "### Softmax Test Case:\n",
    "\n",
    "This test case checks that each row in the resulting softmax probabilities sums to 1, which is the fundamental property of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qL5ToHmkrTr-",
    "outputId": "af1aedc1-61d7-415e-9060-2313cbfde13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax function passed the test case!\n"
     ]
    }
   ],
   "source": [
    "# Example test case\n",
    "z_test = np.array([[2.0, 1.0, 0.1], [1.0, 1.0, 1.0]])\n",
    "softmax_output = softmax(z_test)\n",
    "\n",
    "# Verify if the sum of probabilities for each row is 1 using assert\n",
    "row_sums = np.sum(softmax_output, axis=1)\n",
    "\n",
    "# Assert that the sum of each row is 1\n",
    "assert np.allclose(row_sums, 1), f\"Test failed: Row sums are {row_sums}\"\n",
    "\n",
    "print(\"Softmax function passed the test case!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1uPYyhotoAf"
   },
   "source": [
    "### Prediction Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "8qwCbgC1vyHn"
   },
   "outputs": [],
   "source": [
    "def predict_softmax(X, W, b):\n",
    "    \"\"\"\n",
    "    Predict the class labels for a set of samples using the trained softmax model.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the number of features.\n",
    "    W (numpy.ndarray): Weight matrix of shape (d, c), where c is the number of classes.\n",
    "    b (numpy.ndarray): Bias vector of shape (c,).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Predicted class labels of shape (n,), where each value is the index of the predicted class.\n",
    "    \"\"\"\n",
    "    logits = np.dot(X, W) + b\n",
    "    z_exp = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    softmax_probs = z_exp / np.sum(z_exp, axis=1, keepdims=True)\n",
    "    predicted_classes = np.argmax(softmax_probs, axis=1)\n",
    "\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCGDTavVuXZu"
   },
   "source": [
    "### Test Function for Prediction Function:\n",
    "The test function ensures that the predicted class labels have the same number of elements as the input samples, verifying that the model produces a valid output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "musr99YhucQX",
    "outputId": "0e3beead-0770-4f61-e07c-59ba5375fa98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class labels: [1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Define test case\n",
    "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]])  # Feature matrix (3 samples, 2 features)\n",
    "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]])  # Weights (2 features, 3 classes)\n",
    "b_test = np.array([0.1, 0.2, 0.3])  # Bias (3 classes)\n",
    "\n",
    "# Expected Output:\n",
    "# The function should return an array with class labels (0, 1, or 2)\n",
    "\n",
    "y_pred_test = predict_softmax(X_test, W_test, b_test)\n",
    "\n",
    "# Validate output shape\n",
    "assert y_pred_test.shape == (3,), f\"Test failed: Expected shape (3,), got {y_pred_test.shape}\"\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted class labels:\", y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwejxbajvEle"
   },
   "source": [
    "### Loss Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "bjqnULCtun_Z"
   },
   "outputs": [],
   "source": [
    "def loss_softmax(y_pred, y):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss for a single sample.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred (numpy.ndarray): Predicted probabilities of shape (c,) for a single sample,\n",
    "                             where c is the number of classes.\n",
    "    y (numpy.ndarray): True labels (one-hot encoded) of shape (c,), where c is the number of classes.\n",
    "\n",
    "    Returns:\n",
    "    float: Cross-entropy loss for the given sample.\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = -np.sum(y * np.log(y_pred)) / y.shape[0]\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXdMIV_cz5Fn"
   },
   "source": [
    "## Test case for Loss Function:\n",
    "This test case Compares loss for correct vs. incorrect predictions.\n",
    "*   Expects low loss for correct predictions.\n",
    "*   Expects high loss for incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IhRGquu0N9P",
    "outputId": "89cb7e23-270a-46f3-84bc-f359cdcf9973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss (Correct Predictions): 0.1435\n",
      "Cross-Entropy Loss (Incorrect Predictions): 2.9957\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define correct predictions (low loss scenario)\n",
    "y_true_correct = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # True one-hot labels\n",
    "y_pred_correct = np.array([[0.9, 0.05, 0.05],\n",
    "                           [0.1, 0.85, 0.05],\n",
    "                           [0.05, 0.1, 0.85]])  # High confidence in the correct class\n",
    "\n",
    "# Define incorrect predictions (high loss scenario)\n",
    "y_pred_incorrect = np.array([[0.05, 0.05, 0.9],  # Highly confident in the wrong class\n",
    "                              [0.1, 0.05, 0.85],\n",
    "                              [0.85, 0.1, 0.05]])\n",
    "\n",
    "# Compute loss for both cases\n",
    "loss_correct = loss_softmax(y_pred_correct, y_true_correct)\n",
    "loss_incorrect = loss_softmax(y_pred_incorrect, y_true_correct)\n",
    "\n",
    "# Validate that incorrect predictions lead to a higher loss\n",
    "assert loss_correct < loss_incorrect, f\"Test failed: Expected loss_correct < loss_incorrect, but got {loss_correct:.4f} >= {loss_incorrect:.4f}\"\n",
    "\n",
    "# Print results\n",
    "print(f\"Cross-Entropy Loss (Correct Predictions): {loss_correct:.4f}\")\n",
    "print(f\"Cross-Entropy Loss (Incorrect Predictions): {loss_incorrect:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0d3fm1-vUlY"
   },
   "source": [
    "### Cost Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "yaH9_s0svIGJ"
   },
   "outputs": [],
   "source": [
    "def cost_softmax(X, y, W, b):\n",
    "    \"\"\"\n",
    "    Compute the average softmax regression cost (cross-entropy loss) over all samples.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the number of features.\n",
    "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c), where n is the number of samples and c is the number of classes.\n",
    "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
    "    b (numpy.ndarray): Bias vector of shape (c,).\n",
    "\n",
    "    Returns:\n",
    "    float: Average softmax cost (cross-entropy loss) over all samples.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    logits = np.dot(X, W) + b\n",
    "    y_pred = softmax(logits)\n",
    "    # total_loss = np.sum(loss_softmax(y_pred[i], y[i]) for i in range(n))\n",
    "    total_loss = np.sum(loss_softmax(y_pred, y))\n",
    "    \n",
    "\n",
    "    # Return average loss\n",
    "    return total_loss / n\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eGyPFJ33tgY"
   },
   "source": [
    "### Test Case for Cost Function:\n",
    "The test case assures that the cost for the incorrect prediction should be higher than for the correct prediction, confirming that the cost function behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIGAxYQt36Sr",
    "outputId": "ea1093d8-a9d3-44b2-dfb4-d172adfe0bee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for correct prediction: 0.0003117182066674662\n",
      "Cost for incorrect prediction: 0.14965430679723057\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example 1: Correct Prediction (Closer predictions)\n",
    "X_correct = np.array([[1.0, 0.0], [0.0, 1.0]])  # Feature matrix for correct predictions\n",
    "y_correct = np.array([[1, 0], [0, 1]])  # True labels (one-hot encoded, matching predictions)\n",
    "W_correct = np.array([[5.0, -2.0], [-3.0, 5.0]])  # Weights for correct prediction\n",
    "b_correct = np.array([0.1, 0.1])  # Bias for correct prediction\n",
    "\n",
    "# Example 2: Incorrect Prediction (Far off predictions)\n",
    "X_incorrect = np.array([[0.1, 0.9], [0.8, 0.2]])  # Feature matrix for incorrect predictions\n",
    "y_incorrect = np.array([[1, 0], [0, 1]])  # True labels (one-hot encoded, incorrect predictions)\n",
    "W_incorrect = np.array([[0.1, 2.0], [1.5, 0.3]])  # Weights for incorrect prediction\n",
    "b_incorrect = np.array([0.5, 0.6])  # Bias for incorrect prediction\n",
    "\n",
    "# Compute cost for correct predictions\n",
    "cost_correct = cost_softmax(X_correct, y_correct, W_correct, b_correct)\n",
    "\n",
    "# Compute cost for incorrect predictions\n",
    "cost_incorrect = cost_softmax(X_incorrect, y_incorrect, W_incorrect, b_incorrect)\n",
    "\n",
    "# Check if the cost for incorrect predictions is greater than for correct predictions\n",
    "assert cost_incorrect > cost_correct, f\"Test failed: Incorrect cost {cost_incorrect} is not greater than correct cost {cost_correct}\"\n",
    "\n",
    "# Print the costs for verification\n",
    "print(\"Cost for correct prediction:\", cost_correct)\n",
    "print(\"Cost for incorrect prediction:\", cost_incorrect)\n",
    "\n",
    "print(\"Test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-YIb7zlveKq"
   },
   "source": [
    "### Computing Gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "G3Vpn5bNvW3x"
   },
   "outputs": [],
   "source": [
    "def compute_gradient_softmax(X, y, W, b):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the cost function with respect to weights and biases.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, d).\n",
    "    y (numpy.ndarray): True lab\"els (one-hot encoded) of shape (n, c).\n",
    "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
    "    b (numpy.ndarray): Bias vector of shape (c,).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Gradients with respect to weights (d, c) and biases (c,).\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    logits = np.dot(X, W) + b\n",
    "    y_pred = softmax(logits)\n",
    "    grad_W = np.dot(X.T, (y_pred - y)) / n\n",
    "    grad_b = np.sum(y_pred - y, axis=0) / n\n",
    "\n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S84yoIUx7vY7"
   },
   "source": [
    "### Test case for compute_gradient function:\n",
    "The test checks if the gradients from the function are close enough to the manually computed gradients using np.allclose, which accounts for potential floating-point discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-YSC_Ot70bZ",
    "outputId": "e87fbb1e-6048-419a-ae11-502fa5c0f044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t. W: [[ 0.1031051   0.01805685 -0.12116196]\n",
      " [-0.13600547  0.00679023  0.12921524]]\n",
      "Gradient w.r.t. b: [-0.03290036  0.02484708  0.00805328]\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple feature matrix and true labels\n",
    "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]])  # Feature matrix (3 samples, 2 features)\n",
    "y_test = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  # True labels (one-hot encoded, 3 classes)\n",
    "\n",
    "# Define weight matrix and bias vector\n",
    "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]])  # Weights (2 features, 3 classes)\n",
    "b_test = np.array([0.1, 0.2, 0.3])  # Bias (3 classes)\n",
    "\n",
    "# Compute the gradients using the function\n",
    "grad_W, grad_b = compute_gradient_softmax(X_test, y_test, W_test, b_test)\n",
    "\n",
    "# Manually compute the predicted probabilities (using softmax function)\n",
    "z_test = np.dot(X_test, W_test) + b_test\n",
    "y_pred_test = softmax(z_test)\n",
    "\n",
    "# Compute the manually computed gradients\n",
    "grad_W_manual = np.dot(X_test.T, (y_pred_test - y_test)) / X_test.shape[0]\n",
    "grad_b_manual = np.sum(y_pred_test - y_test, axis=0) / X_test.shape[0]\n",
    "\n",
    "# Assert that the gradients computed by the function match the manually computed gradients\n",
    "assert np.allclose(grad_W, grad_W_manual), f\"Test failed: Gradients w.r.t. W are not equal.\\nExpected: {grad_W_manual}\\nGot: {grad_W}\"\n",
    "assert np.allclose(grad_b, grad_b_manual), f\"Test failed: Gradients w.r.t. b are not equal.\\nExpected: {grad_b_manual}\\nGot: {grad_b}\"\n",
    "\n",
    "# Print the gradients for verification\n",
    "print(\"Gradient w.r.t. W:\", grad_W)\n",
    "print(\"Gradient w.r.t. b:\", grad_b)\n",
    "\n",
    "print(\"Test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W75VL71ivpjG"
   },
   "source": [
    "### Implementing Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "bbQ7SVw7vo-M"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_softmax(X, y, W, b, alpha, n_iter, show_cost=False):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the weights and biases.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, d).\n",
    "    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
    "    W (numpy.ndarray): Weight matrix of shape (d, c).\n",
    "    b (numpy.ndarray): Bias vector of shape (c,).\n",
    "    alpha (float): Learning rate.\n",
    "    n_iter (int): Number of iterations.\n",
    "    show_cost (bool): Whether to display the cost at intervals.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Optimized weights, biases, and cost history.\n",
    "    \"\"\"\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Compute gradients\n",
    "        grad_W, grad_b = compute_gradient_softmax(X, y, W, b)\n",
    "        # Your Code Here\n",
    "        # Update weights and biases using gradient descent\n",
    "        W -= alpha * grad_W\n",
    "        b -= alpha * grad_b\n",
    "        \n",
    "        # Compute cost (optional for tracking progress)\n",
    "        cost = compute_cost_softmax(X, y, W, b)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Show cost at intervals\n",
    "        if show_cost and i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost = {cost}\")\n",
    "    return W, b, cost_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBG9uSWKHDgX"
   },
   "source": [
    "## Preparing Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "prZ_zAvLpodE"
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_mnist(csv_file, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Reads the MNIST CSV file, splits data into train/test sets, and plots one image per class.\n",
    "\n",
    "    Arguments:\n",
    "        csv_file (str): Path to the CSV file containing MNIST data.\n",
    "        test_size (float): Proportion of the data to use as the test set (default: 0.2).\n",
    "        random_state (int): Random seed for reproducibility (default: 42).\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Split dataset.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\"mnist_dataset.csv\")\n",
    "\n",
    "    # Separate labels and features\n",
    "    y = df.iloc[:, 0].values  # First column is the label\n",
    "    X = df.iloc[:, 1:].values  # Remaining columns are pixel values\n",
    "\n",
    "    # Normalize pixel values (optional but recommended)\n",
    "    X = X / 255.0  # Scale values between 0 and 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Plot one sample image per class\n",
    "    plot_sample_images(X, y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def plot_sample_images(X, y):\n",
    "    \"\"\"\n",
    "    Plots one sample image for each digit class (0-9).\n",
    "\n",
    "    Arguments:\n",
    "        X (np.ndarray): Feature matrix containing pixel values.\n",
    "        y (np.ndarray): Labels corresponding to images.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    unique_classes = np.unique(y)  # Get unique class labels\n",
    "    for i, digit in enumerate(unique_classes):\n",
    "        index = np.where(y == digit)[0][0]  # Find first occurrence of the class\n",
    "        image = X[index].reshape(28, 28)  # Reshape 1D array to 28x28\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Digit: {digit}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "ZtYR42Qas2uf",
    "outputId": "5e813934-8272-476e-eb15-f8c87a91572f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'mnist.csv' with the path to your MNIST CSV file\n",
    "    csv_file = 'mnist.csv'\n",
    "    X_train, X_test, y_train, y_test = load_and_prepare_mnist(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyMBH4mQtzHA"
   },
   "source": [
    "### **A Quick debugging Step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIJhtnuCs7QF",
    "outputId": "994cc1d0-3098-4f53-95c9-8692811d519d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move forward: Dimension of Feture Matrix X and label vector y matched.\n"
     ]
    }
   ],
   "source": [
    "# Assert that X and y have matching lengths\n",
    "assert len(X_train) == len(y_train), f\"Error: X and y have different lengths! X={len(X_train)}, y={len(y_train)}\"\n",
    "print(\"Move forward: Dimension of Feture Matrix X and label vector y matched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TKIsKJcwFsv"
   },
   "source": [
    "## **Train the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEuTbCU0xAQW",
    "outputId": "832c9c89-12d8-40c5-9465-3477d424121b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (48000, 784)\n",
      "Test data shape: (12000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "J8e2mHmRv4fd",
    "outputId": "ede7dccf-93a2-4153-8c72-b6051b0007d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 2.200877613970629\n",
      "Iteration 100: Cost = 0.6073976446500978\n",
      "Iteration 200: Cost = 0.4894724052102171\n",
      "Iteration 300: Cost = 0.440839091232275\n",
      "Iteration 400: Cost = 0.4127599937986024\n",
      "Iteration 500: Cost = 0.3938988554973262\n",
      "Iteration 600: Cost = 0.38008476974734245\n",
      "Iteration 700: Cost = 0.36938511491936005\n",
      "Iteration 800: Cost = 0.3607675005445047\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Check if y_train is one-hot encoded\n",
    "if len(y_train.shape) == 1:\n",
    "    encoder = OneHotEncoder(sparse_output=False)  # Use sparse_output=False for newer versions of sklearn\n",
    "    y_train = encoder.fit_transform(y_train.reshape(-1, 1))  # One-hot encode labels\n",
    "    y_test = encoder.transform(y_test.reshape(-1, 1))  # One-hot encode test labels\n",
    "\n",
    "# Now y_train is one-hot encoded, and we can proceed to use it\n",
    "d = X_train.shape[1]  # Number of features (columns in X_train)\n",
    "c = y_train.shape[1]  # Number of classes (columns in y_train after one-hot encoding)\n",
    "\n",
    "# Initialize weights with small random values and biases with zeros\n",
    "W = np.random.randn(d, c) * 0.01  # Small random weights initialized\n",
    "b = np.zeros(c)  # Bias initialized to 0\n",
    "\n",
    "# Set hyperparameters for gradient descent\n",
    "alpha = 0.1  # Learning rate\n",
    "n_iter = 1000  # Number of iterations to run gradient descent\n",
    "\n",
    "# Train the model using gradient descent\n",
    "W_opt, b_opt, cost_history = gradient_descent_softmax(X_train, y_train, W, b, alpha, n_iter, show_cost=True)\n",
    "\n",
    "# Plot the cost history to visualize the convergence\n",
    "plt.plot(cost_history)\n",
    "plt.title('Cost Function vs. Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH4wNbhzys4f"
   },
   "source": [
    "## **Evaluating the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzV7BkRqOl5A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance using confusion matrix, precision, recall, and F1-score.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): True labels\n",
    "    y_pred (numpy.ndarray): Predicted labels\n",
    "\n",
    "    Returns:\n",
    "    tuple: Confusion matrix, precision, recall, F1 score\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    return cm, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uuGtvIlywK7J",
    "outputId": "1aa26874-665c-4d58-af5b-560f62c51d16"
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_test = predict_softmax(X_test, W_opt, b_opt)\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # True labels in numeric form\n",
    "\n",
    "# Evaluate the model\n",
    "cm, precision, recall, f1 = evaluate_classification(y_test_labels, y_pred_test)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Visualizing the Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "cax = ax.imshow(cm, cmap='Blues')  # Use a color map for better visualization\n",
    "\n",
    "# Dynamic number of classes\n",
    "num_classes = cm.shape[0]\n",
    "ax.set_xticks(range(num_classes))\n",
    "ax.set_yticks(range(num_classes))\n",
    "ax.set_xticklabels([f'Predicted {i}' for i in range(num_classes)])\n",
    "ax.set_yticklabels([f'Actual {i}' for i in range(num_classes)])\n",
    "\n",
    "# Add labels to each cell in the confusion matrix\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='white' if cm[i, j] > np.max(cm) / 2 else 'black')\n",
    "\n",
    "# Add grid lines and axis labels\n",
    "ax.grid(False)\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.colorbar(cax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv8J5hNCPzl6"
   },
   "source": [
    "# Linear Seperability and Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTyaPcM-P69S"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate linearly separable dataset\n",
    "X_linear_separable, y_linear_separable = make_classification(\n",
    "    n_samples=200, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_linear, X_test_linear, y_train_linear, y_test_linear = train_test_split(\n",
    "    X_linear_separable, y_linear_separable, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train logistic regression model on linearly separable data\n",
    "logistic_model_linear_separable = LogisticRegression()\n",
    "logistic_model_linear_separable.fit(X_train_linear, y_train_linear)\n",
    "\n",
    "# Generate non-linearly separable dataset (circles)\n",
    "X_non_linear_separable, y_non_linear_separable = make_circles(\n",
    "    n_samples=200, noise=0.1, factor=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_non_linear, X_test_non_linear, y_train_non_linear, y_test_non_linear = train_test_split(\n",
    "    X_non_linear_separable, y_non_linear_separable, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train logistic regression model on non-linearly separable data\n",
    "logistic_model_non_linear_separable = LogisticRegression()\n",
    "logistic_model_non_linear_separable.fit(X_train_non_linear, y_train_non_linear)\n",
    "\n",
    "# Plot decision boundaries for linearly and non-linearly separable data\n",
    "def plot_decision_boundary(ax, model, X, y, title):\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot decision boundary for linearly separable data (Training)\n",
    "plot_decision_boundary(axes[0, 0], logistic_model_linear_separable, X_train_linear, y_train_linear,\n",
    "                       'Linearly Separable Data (Training)')\n",
    "\n",
    "# Plot decision boundary for linearly separable data (Testing)\n",
    "plot_decision_boundary(axes[0, 1], logistic_model_linear_separable, X_test_linear, y_test_linear,\n",
    "                       'Linearly Separable Data (Testing)')\n",
    "\n",
    "# Plot decision boundary for non-linearly separable data (Training)\n",
    "plot_decision_boundary(axes[1, 0], logistic_model_non_linear_separable, X_train_non_linear,\n",
    "                       y_train_non_linear, 'Non-Linearly Separable Data (Training)')\n",
    "\n",
    "# Plot decision boundary for non-linearly separable data (Testing)\n",
    "plot_decision_boundary(axes[1, 1], logistic_model_non_linear_separable, X_test_non_linear,\n",
    "                       y_test_non_linear, 'Non-Linearly Separable Data (Testing)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plots as PNG files\n",
    "plt.savefig('decision_boundaries.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda]",
   "language": "python",
   "name": "conda-env-.conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
